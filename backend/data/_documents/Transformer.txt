The transformer is a widely used neural network architecture in text processing. It was introduced in the paper 'Attention is all you need'. There are two main parts in the transformer.
Encoder
This part of the transformer takes in the input string of characters and converts them to vectors while using a self attention mechanism.
Embedding
When a batch of tokens is first sent in, in shape (batch size, sequence length), they need to be embedded with some embedding dimension to convert each token to a embed-dim-sized vector. This results in a (batch size, sequence length, embed dims) tensor.
Positional Encoding
Since the order of the tokens matters, a same-sized tensor is added to the embeddings to give them positional value differences that are recognizable by the model. The functions for the positional encoding tensors are based off of sine and cosine.
Feed-Forward
The tensor of shape (batch size, sequence length, embed dims) is fed into a feed forward layer that maps the embed dims to three times the embed dims, making a shape of 
(batch size, sequence length, 3 * embed dims). This is split into 3 over the last dimension to get three tensors of the original size. These are called the query, key, and value tensors. The queries represent the attention towards other tokens and the keys represent the different tokens that could be paid attention to. The values represent the actual information that each token holds at this point.
Multi-Headed Self-Attention
For each head of attention, an equal cut of the query, key, and value tensors is taken along the last dimension so they each have a shape of (batch size, sequence length, embed size / # heads). The query tensor is multiplied with the transpose of the key tensor to get a shape of 
(batch size, sequence length, sequence length). This multiplication makes it so that each token is being compared with every other token, forming an attention matrix. This means that the model can figure out how much attention each token is paying to each other token. The tensor may be scaled at this point to prevent explosion of values. A padding mask tensor of the same size is added to try to remove the activations of padding tokens. This is usually done by adding a negative number that is very close to 0. The Softmax function is then applied over the last dimension ([[Activation Functions]]). To get probabilities in the attention matrix representing how much attention is paid to each token by each token. The value tensor is multiplied by this to quantify how much of the values of each token are taken in by each token based on the attention matrix. This leaves us with a tensor of shape (batch size, sequence length, embed size / # heads). This process is done for each cut of the query, key, and value tensors and then all of the resulting tensors are concatenated across the last dimension bringing the shape back to
(batch size, sequence length, embed dims).
Residual Tensor and Layer Normalization
 A residual tensor is added (which is the input to the self-attention). The tensor is normalized across the last dimension (layer norm. not batch norm.). This is done by first finding the mean and variance for each feature. Each vector of size (embed dims) is normalized by subtracting the mean of that vector, and dividing by the square root of the variance of that feature plus a small constant to avoid division by 0.
Feed-Forward
This tensor of size (batch size, sequence length, embed dims) is passed to a feed forward layer with a variable number of features. It is then passed back through another feed forward to retain the original shape. The residual from before the two feed forward networks is added and the tensor is normalized the same way as it was after the self-attention. This is the output of the encoder part of the transformer.
Decoder
This part of the transformer is responsible for taking in the context and combining it with the vectors from the encoder to form a prediction. In training, the actual desired outputs would be given to help teach the model. In inference, only the start token would be given.
Positional Encoding and Feed-Forward
The input is the embedded tokens for the output (assume that model is being trained), so it would have a shape of (batch size, sequence length, embed dims). A positional encoding tensor is added just like in the encoder, and then it is passed into a feed forward that get queries, keys, and values just like in the encoder.
Masked Multi-Headed Self-Attention, Residuals, and Layer Norm.
The query, keys, and value are split the and processed in the same way as in the encoder, forming an attention matrix in each head of attention, up until the masking. Here, instead of just adding a padding mask, a look ahead mask is also added. This mask is used to make sure that the model is not using the outputs that are passed in for teaching. Any token should not be able to access the the tokens after it in order to find the tokens it should attend to. The future values should be set such that the activation is close to 0 after SoftMax is applied. After this, the self-attention continues in the same way as the encoder, producing an output of shape (batch size, sequence length, embed dims) after each attention head is concatenated. Similarly to the encoder, the residual tensor is added and the tensor is normalized across the last dimension.
Multi-Headed Cross-Attention
This is when the output from the encoder, and the tensor from the decoder so far meet. Firstly, the output from the encoder, which has shape (batch size, sequence length, embed dims) will be put through a feed forward network to make it of shape (batch size, sequence length, 2 * embed dims). This will be split into two tensors of the original shape which are the key and value tensors. The queries from the decoder and the key and values from the encoder are combined in multi-headed cross-attention. The query and the transpose of the keys are multiplied like usual and the padding mask is added after scaling the tensor. The look ahead mask is not added because in this stage, each token should have access to all of the tokens from the input. SoftMax is performed like usual and the value is multiplied to get the output of one attention head. The results of each head are concatenated like usual to get a shape of (batch size, sequence length, embed dims). The residuals are added and the tensor is layer normalized again. The tensor is put into a final feed forward network that doesn't change the shape, and is added and normalized one last time.
Feed-Forward
The tensor of shape (batch size, sequence length, embed size) is fed into a feed forward network to map the final dimension to vocab. size. This can then be put through SoftMax to get probability distributions for the next token.