Long short term memory or LSTM, is a type of neural network that is common for time series regression tasks. It builds upon recurrent neural networks, which faced the vanishing gradient problem, where the gradients of weights in the neural network approach 0, making it so that the network doesn't learn as efficiently. Instead of only having a hidden state that is different across timesteps like an RNN, LSTMs have a cell state that is meant to represent long term patterns and is passed forward throughout the forward pass with small changes, unlike the hidden state.

Equations
Variables

| Variable name  | Interpretation                                                | Shape                                        |
| -------------- | ------------------------------------------------------------- | -------------------------------------------- |
| X              | set of input timeframes going in                              | (batch size, # timeframes,       # features) |
| H              | vector of hidden state                                        | (batch size, # units)                        |
| C              | vector of cell state                                          | (batch size, # units)                        |
| WF, WI, WC, WO | weight matrices that are applied to the input of a process    | (# features, # units)                        |
| UF, UI, UC, UO | weight matrices that are applied to the previous hidden state | (# units, # units)                           |
| BF, BI, BC, BO | bias vectors that are added to the weighted sums              | (# units)                                    |
[[Activation Functions]] used:

| Function | Name    |
| -------- | ------- |
| Sigmoid  | $$σ_g$$ |
| TanH     | $$σ_c$$ |

Logic
Like an RNN, each input in X is put into a different process, where each process is influenced by the output of the previous process (the hidden state), and the cell state, making the model temporally sensitive. These are the equations:
$$
f_t = σ_g(W_f * X_t + U_f * H_t-1 + B_f)
$$
$$
i_t = σ_g(W_i * X_t + U_i * H_t-1 + B_i)
$$
$$
c'_t = σ_c(W_c * X_t + U_c * H_t-1 + B_c)
$$
$$
o_t = σ_g(W_o * X_t + U_o * H_t-1 + B_o)
$$
$$
c_t = f_t * c_t-1 + i_t * c'_t
$$
$$
h_t = o_t * σ_c(c_t)
$$
All of the equations follow the same pattern of multiplying input and recurrent weights with the input and hidden state, and adding the bias. 

How they each contribute to the new hidden and cell state is what makes the LSTM unique. As you can see, the first equation uses the sigmoid function to get a probability, which ends up being multiplied by the cell state, meaning it represents the amount of cell state that is "forgotten". The second equation is also a probability, and it gets multiplied by the third equation and added to the cell state, which means the second and third equations represent what gets added to the cell state. 

The new hidden state is also a product of the fourth equation, which is a probability, and the TanH of the cell state, which is the value that's being scaled.

It it recommended to put the shapes of the matrices and vectors in to the formulas to better understand how it works.

After this processing, the hidden state is passed to the next timeframe and the process repeats for however many timeframes are given in the input. After the final timeframe is processed, the hidden state represents the output of the LSTM. In some LSTMs not only is the final hidden state returned, but all of the hidden states that were processed are returned as well. This depends on the use case of the LSTM.

Note: The output of the LSTM (only the final hidden state) is a vector of size (# units). This will not always be equal to the number of features, which is why it is put through a fully connected layer afterwards in order to get the actual output.

