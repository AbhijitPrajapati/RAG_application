An activation function is a function that, in theory, is applied to the output of each neuron in a network, in order to introduce value ranges that are essential for the neural network to run well, and remove value ranges that give way to bias in a network. There are many different types of activation functions.
ReLu
Rectified Linear Unit is an activation function that maps all inputs less than 0 to 0, and all inputs above 0, to themselves. It is essentially the same as no activation function, but with no negative numbers. This function is great for introducing sparsity because it zeros out negative neurons, making them inactive. This is why it is a great choice for hidden layers of a neural network, so that overfitting can be reduced. This activation function does have problems with negative numbers, which can lead to gradients vanishing as the gradient of this function is 0 when the input is negative. For this reason, it is mostly used outside hidden layers for things like predicting stock pricing, which is a regression task. Because stock prices are always positive and continuous, the  ReLu function is a great choice for it.

Sigmoid
The sigmoid function is very useful for binary classification, as it is a smooth continuous function ranging from 0 to 1. The output of this function will often represent the "confidence" of the output that the neural network has given. This function should not be used in cases where very high or very low values are wanted, because the gradient vanishes as the values get closer and closer to 0 or 1 as shown below.

TanH
This activation function is similar to the sigmoid function in shape, but it has one key difference: It is zero centered. This means that the mean of the output range, -1 to 1, is 0. It is commonly used in hidden layers of networks and doesn't face the vanishing gradient problem to the same extent as other functions, due to its wider scope. It is ideal for tasks that include negative and positive outputs.

Softmax
Softmax has a similar purpose to the sigmoid function, but is used for multiclass classification. One of its properties is that all of the outputs in the output vector, (which are in between 0 and 1) will always add up to 1, making it useful for multiclass classification. It is mostly only used at the end of a network to compile all of the results into a probabalistic vector. 